---
title: "Missing data methods II"
author: "Dr. Olanrewaju Michael Akande"
date: "Oct 29, 2019"
output:
  xaringan::moon_reader:
    css: "slides.css"
    logo: img/class_logo.png
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
#library(tidyverse)
#library(magick)
library(knitr)
library(kableExtra)
library(lattice)
#library(dplyr)
#library(ggplot2)
#library(lme4)
#library(arm)
library(mice)
library(VIM)
```

```{r setup, include=FALSE}
# R options
options(
  htmltools.dir.version = FALSE, # for blogdown
  #show.signif.stars = FALSE,     # for regression output
  warm = 1
  )
# Set dpi and height for images
knitr::opts_chunk$set(fig.height = 2.65,dpi =300,fig.align='center',fig.show='hold',size='footnotesize',small.mar=TRUE) 
# For nonsese...
htmltools::tagList(rmarkdown::html_dependency_font_awesome())
# For magick
dev.off <- function(){
  invisible(grDevices::dev.off())
}

knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar =  c(4, 4, 1.5, 1.5)) 
})
```

## Announcements

- Instructions for project proposals are on the course website.

## Outline

- Multiple imputation

  + Joint Modeling
  
  + Sequential regression models
  
  + General advice
  
  + Simple illustration


---
class: center, middle

# Multiple imputation


---
## MI recap

- Fill in dataset $m$ times with imputations.

--

- Analyze repeated datasets separately, then combine the estimates from each one.

--

- Imputations drawn from probability models for missing data.

--

```{r echo=FALSE, out.height="370px"}
knitr::include_graphics("img/MultipleImp.png")
```


---
## MI recap

Rubin (1987)

- Population estimand: $Q$

- Sample estimate: $q$

- Variance of $q$: $u$

- In each imputed dataset $d_j$, where $j = 1,\ldots,m$, calculate
$$q_j = q(d_j)$$
$$u_j = u(d_j).$$




---
## MI recap

- MI estimate of $Q$:
.block[
$$\bar{q}_m = \sum\limits_{i=1}^m \dfrac{q_i}{m}.$$
]

--

- MI estimate of variance is:
.block[
$$T_m = (1+1/m)b_m + \bar{u}_m.$$
]

  where
.block[
$$b_m = \sum\limits_{i=1}^m \dfrac{(q_i - \bar{q}_m)^2}{m-1}; \ \ \ \ \bar{u}_m = \sum\limits_{i=1}^m \dfrac{u_i}{m}.$$
]
--
	
- Use t-distribution inference for $Q$
.block[
$$\bar{q}_m \pm t_{1-\alpha/2} \sqrt{T_m}.$$
]


---
## Explanation of MI variance

- MI estimate of variance is:
.block[
$$T_m = (1+1/m)b_m + \bar{u}_m.$$
]

- According to the law of total variance, for two random variables $X$ and $Y$,
.block[
$$\mathbb{Var}[Y] = \mathbb{E}[\mathbb{Var}[Y|X]] + \mathbb{Var}[\mathbb{E}[Y|X]].$$
]

  which implies that 
.block[
$$\mathbb{Var}[Y|Z] = \mathbb{E}[\mathbb{Var}[Y|Z,X]] + \mathbb{Var}[\mathbb{E}[Y|Z,X]],$$
]
  for a third random variable $Z$.

- Consider $m = \infty$, where $m$ is the number of completed datasets, Rubin (1987) shows
.block[
$$\mathbb{Var}[Q | \boldsymbol{Y}_{obs}] = \mathbb{E}[\mathbb{Var}[Q|\boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis}]] + \mathbb{Var}[\mathbb{E}[Q|\boldsymbol{Y}_{obs},\boldsymbol{Y}_{mis}]] = b_\infty +  \bar{u}_\infty,$$
]

  and one can also condition on $\boldsymbol{X}$ in the regression context where predictors are available.


---
## MI: pros and cons

- Advantages

--

  + Straightforward estimation of uncertainty

--
  
  + Flexible modeling of missing data
	
--

- Disadvantages (??)

--

  + Extra data sets to manage
 
--
 
  + Explicitly model-based
  
  
---
## Resources for learning more

- Little and Rubin (2002), *Statistical Analysis with Missing Data*, Wiley

- Schafer (1997), *Analysis of Incomplete Multivariate Data*, CRC Press

- Reiter and Raghunathan (2007), "The multiple adaptations of multiple imputation," *JASA*.


---
class: center, middle

# Where should the imputations come from?


---
## MI: where should the imputations come from?

So where should we get reasonable replacements for the missing values from? There are two general approaches:

--

- Sequential modeling

--

  + Estimate a sequence of conditional models (think separate regressions for each variable!);
 
--
 
  + Impute from each model.

--
  
- Joint modeling

--

  + Choose a multivariate model for all the data (we will not cover joint multivariate models in this class; consider taking STA602!);
   
--

  + Estimate the model;
  
--

  + Impute from the joint model.
  

---
## MI: sequential regression models

Suppose the data includes three variables $Y_1$, $Y_2$, $Y_3$. 

--

+ Step 1: fill in missing values by simulating values from regressions based on complete cases;
 
--
 
+ Step 2: regress $Y_1|Y_2,Y_3$ using completed data;
 
--
 
+ Step 3: impute new values of $Y_1$ from this model;

--
  
+ Step 4: repeat for $Y_2|Y_1,Y_3$ and $Y_3|Y_1,Y_2$ (repeat for all variables with missing data);

--
  
+ Step 5: cycle through Steps 1 to Step 4 many times;
  - Usually 5 times is a default but there is not theory underpinning this default.
 
--
 
Final dataset is one imputed dataset. Repeat entire process $m$ times to get $m$ multiply-imputed datasets.


---
## Existing software for sequential modeling

Free software packages

  + MICE for R and Stata (so many conditional models to pick from, for example, predictive mean matching, random forest, linear regression, logistic regression, and so on);
  
  + statsmodels MICE in python (only uses predictive mean matching);
  
  + MI for R;
  
  + IVEWARE for SAS.

In sequential modeling, one can specify many types of conditional models and include constraints on values.


---
## Existing software for joint modeling

- Multivariate normal data
  + R: NORM, Amelia II;
  
  + SAS: proc MI;
  
  + Stata: MI command.
  
- Mixtures of multivariate normal distributions
  + R: EditImpCont (also does editing of faulty values).
  
- Multinomial data:
  + R: CAT (log-linear model), NPBayesImpute (latent class model).
  

---
## Existing software for joint modeling

- Nested Multinomial data:
  + R: NestedCategBayesImpute (also generates synthetic data).  
      *update coming soon to allow for editing of faulty values*
  
- Mixed data:
  + R: MIX (general location model).
  
- Many other joint models, but often without open source software.



---
## Comparing sequential to joint modeling

Advantages

--

- Often easier to specify reasonable conditionals than a joint model.

--

- Complex MCMC not needed.

--

- Can use machine learning methods for conditionals.

--

Disadvantages

--

- Labor intensive to specify models.

--

- Incoherent conditionals can cause odd behaviors (e.g., order matters).

--

- Theoretical properties difficult to assess.



---
## What if imputation and analysis model do not match?

- Imputation model more general than analysis model: .hlight[conservative inferences].

--

- Imputation model less general than analysis model: .hlight[invalid inferences].

--

- For sequential modeling, include all variables related to outcome and missing data (Schafer 1997).

--

- Include design information in models (Reiter *et al.* 2006, *Survey Methodology*).



---
## Evaluating the fit of imputation models

- Plots of imputed and observed values (Abayomi *et al*, 2008, *JRSS-C*)
  + Imputed values that don't look like the observed values could *maybe* imply poor imputation models;

--
  
  + Useful as a sensibility check
  
- Model-specific diagnostics (Gelman *et al*,  2005, *Biometrics*)
  + Take a look at residual plots with marked observed and imputed values;

--
  
  + Look for obvious abnormalities.


---
## Remarks

- Ignoring missing data is risky.

--

- Single imputation procedures at best underestimate uncertainty and at worst fail to capture multivariate relationships.

--

- Multiple imputation recommended (or other model-based methods).

--

- We discussed MI for MAR data. When data are NMAR life much harder -- get experts in missing data on your team.



---
## Remarks

- Incorporate all sources of uncertainty in imputations, including uncertainty in parameter estimates.

--

- Want models that accurately describe the distribution of missing values.

--

- Important to keep in mind that imputation model used only for cases with missing data.

--

  + Suppose you have 30% missing values;

--
  
  + Also, suppose your imputation model is "80% good" ("20% bad");

--
  
  + Then, completed data are only "6% bad"!
  

---
class: center, middle

# MI in R


---
## Illustration

- Simple example using data that come with the .hlight[MICE] package in R.

--

- Dataset from NHANES includes 25 cases measured on 4 variables.

--

- Only 13 cases with complete data.

--

- We will use multiple imputation to make completed datasets and do analyses.

--

- The four variables are
  1. age (age group: 20-39, 40-59, 60+)
  2. bmi (body mass index, in $kg/m^2$)
  3. hyp (hypertension status: no, yes)
  4. chl (total cholesterol, in $mg/dL$)


---
## Illustration

```{r}
library(mice)
data(nhanes2)
dim(nhanes2)
summary(nhanes2)
str(nhanes2)
```


---
## Patterns of missing data

```{r eval=F}
md.pattern(nhanes2)
```
```{r echo=FALSE, out.height="400px"}
knitr::include_graphics("img/miss-data-pattern.png")
```

--

5 patterns observed from $2^3=8$ possible patterns


---
## Patterns of missing data

```{r echo=FALSE, out.height="350px"}
knitr::include_graphics("img/miss-data-pattern.png")
```

--

- First column gives number of rows with each pattern, e.g., 3 cases where only cholesterol is missing.

--

- Last column gives number of variables missing in each pattern.

--

- Last row gives total number of missing values by variables.


---
## Visualizing patterns of missing data

```{r fig.height=3.5}
library(VIM)
library(lattice)
aggr(nhanes2,col=c("lightblue3","darkred"),numbers=TRUE,sortVars=TRUE,labels=names(nhanes2),
                    cex.axis=.7,gap=3,ylab=c("Proportion missing","Missingness pattern"))
```


---
## Visualizing patterns of missing data

The .hlight[marginplot] function can be used to understand how missingness affects the distribution of values on other variables.

--

- Blue boxplots summarize the distribution of observed data given the other variable is observed.

--

- Red box plots summarize the distribution of observed data given the other variable is missing

--

- If data are MCAR, you expect the boxplots to be the same (hard to evaluate in this small sample)

--

Let's look at the margin plot for the two continuous variables `bmi` and `chl`.


---
## Visualizing patterns of missing data

```{r fig.height=4.5}
marginplot(nhanes2[,c("chl","bmi")],col=c("lightblue3","darkred"),cex.numbers=1.2,pch=19)
```


---
## Visualizing patterns of missing data

- Interpretation of the numbers in red.

--

  + 9 = number of observations with missingness in `bmi`
  
  + 10 = number of observations with missingness in `chl`
  
  + 7 = number of observations with missingness in both `bmi` and `chl`.

--
  
- The scatterplot of blue points display the relationship between `bmi` and `chl` when they are both observed (13 cases).

--

- The red points indicate the amount of data used to generate the red boxplots.


---
## Mice in R

We will use the .hlight[mice] function to generate 10 imputed datasets. By default, .hlight[mice] uses

- .hlight[pmm]: Predictive mean matching for numeric data

- .hlight[logreg]: Logistic regression for factor data with 2 levels

- .hlight[polyreg]: Multinomial logistic regression for factor data with > 2 levels

- .hlight[polr]: Proportional odds model for factor data with > 2 ordered levels


---
## Mice in R

Other commonly used methods are

- .hlight[norm]: Bayesian linear regression

- .hlight[sample]: Random sample from observed values

- .hlight[cart]: Classification and regression trees

- .hlight[rf]: Random forest

Personally, I prefer to use .hlight[norm] instead of .hlight[pmm] for imputing numeric/continuous variables. For the illustration,

```{r echo=F}
set.seed(30)
```
```{r fig.height=4.5}
nhanes2_imp <- mice(nhanes2,m=10,defaultMethod=c("norm","logreg","polyreg","polr"),print=F)
```

---
## Mice in R

```{r fig.height=4.5}
methods(mice)
```


---
## Predictive mean matching (pmm)

- Suppose $y$ is subject to missing values while $x$ is completely observed. The basic idea for pmm is:

--
 
  + Using complete cases, regress $y$ on $x$, obtaining $\hat{\beta} = (\hat{\beta}_0,\hat{\beta}_1)$;
  
--
 
  + Draw a new $\beta^\star$ from the posterior distribution of $\hat{\beta}$ (e.g, multivariate normal);
  
--
 
  + Using $\beta^\star$, generate predicted values of $y$ for all cases;
  
--
 
  + For each case with a missing $y$, identify set of donors with no missing values, who have predicted $y$ values close to that of the case with missing data;
  
--
 
  + From among these cases, randomly select one and assign its observed value of $y$ as the imputed value;
  
--
 
  + Repeat for all observations and imputation data sets.
  
--
 
- Pmm matches the distribution of the original observed variable, as imputed values are taken from the real data.


---
## Mice in R

Back to the `nhanes2_imp` object, first look at the original data
```{r}
nhanes2
```



---
## Mice in R

Look at the first imputed-dataset
```{r}
d1 <-  complete(nhanes2_imp, 1); d1
```


---
## Mice in R

Look at the last imputed-dataset
```{r}
d10 <-  complete(nhanes2_imp, 10); d10
```


---
## Illustration

Let's plot imputed and observed values for continuous variables.

```{r fig.height=3.5}
stripplot(nhanes2_imp, col=c("grey","darkred"),pch=c(1,20))
```

Grey dots are observed values and red dots are imputed values.

---
## Illustration

Let's see how this would change when we use `pmm` instead of `norm`.
```{r fig.height=3.5}
nhanes2_imp2 <- mice(nhanes2,m=10,defaultMethod=c("pmm","logreg","polyreg","polr"),print=F)
stripplot(nhanes2_imp2, col=c("grey","darkred"),pch=c(1,20))
```

Easy to see that the distribution of the original observed data is preserved.


---
## Illustration

Also can do plots by values of categorical variable, say `bmi` by `age`. Let's look at the imputations using `norm`
```{r fig.height=3.5}
stripplot(nhanes2_imp, bmi~.imp|age, col=c("grey","darkred"),pch=c(1,20))
```


---
## Illustration

Using `pmm` instead of `norm`, we have:
```{r fig.height=3.5}
stripplot(nhanes2_imp2, bmi~.imp|age, col=c("grey","darkred"),pch=c(1,20))
```

Going forward, let's focus only on imputations using `norm`.

---
## Illustration

Scatterplot of `chl` and `bmi` for each imputed dataset. Here we can see why we should not use single imputations. The strength of the relationship can vary based on different imputed values.
```{r fig.height=4.2}
xyplot(nhanes2_imp, bmi ~ chl | .imp,pch=c(1,20),cex = 1.4,col=c("grey","darkred"))
```


---
## Illustration

To detect interesting differences in distribution between observed and imputed data, use the .hlight[densityplot] function.

```{r fig.height=3.6}
densityplot(nhanes2_imp)
```


---
## Illustration: using a single dataset

For model specification, i.e., transformations, either look at the complete cases or use one of the completed datasets. For example, to use the first dataset in a regression of `bmi` on `age`, `hyp` and `chl`, use
```{r fig.height=3.6}
bmiregd1 <- lm(bmi~age+hyp+chl, data = d1)
summary(bmiregd1)
```


---
## Illustration: using a single dataset

- To check residuals, you can examine the fit of the model in one or more completed datasets

- Any transformations will have to apply to all the datasets, so don't be too dataset-specific in your checks.
  ```{r fig.height=3}
plot(bmiregd1$residual,x=d1$chl,xlab="Cholesterol",ylab="Residual"); abline(0,0)
```

  Looks good!

---
## Illustration: using a single dataset

```{r fig.height=3.3}
boxplot(bmiregd1$residual ~ d1$age, xlab = "Age", ylab = "Residual")
```

Pretty reasonable especially given the size of the dataset.


---
## Illustration: using a single dataset

```{r fig.height=3.3}
boxplot(bmiregd1$residual ~ d1$hyp, xlab = "Hypertension", ylab = "Residual")
```

- Good idea to repeat for more than one completed dataset. 

- If you decide transformations are needed, you might reconsider the imputation models too and fit them with transformed values.


---
## Illustration: using all $m$ datasets

```{r fig.height=3.3}
bmireg_imp <- with(data=nhanes2_imp, lm(bmi~age+hyp+chl))
#results for second dataset
bmireg_imp[[4]][[2]]
#results for fifth dataset
bmireg_imp[[4]][[5]]
```


---
## Illustration: using all $m$ datasets

Now to get the multiple imputation inferences based on the Rubin (1987) combining rules
```{r fig.height=3.3}
bmireg <- pool(bmireg_imp)
summary(bmireg)
```


---
## Illustration: using all $m$ datasets

- You can still do a nested F test (well, technically a test that is asymptotically equivalent to a nested F test) for the multiply-imputed dataset using the .hlight[pool.compare] function. 

- For example, suppose we want to see if age is a useful predictor, then
  ```{r fig.height=3.3}
bmireg_imp <- with(data=nhanes2_imp, lm(bmi~hyp+chl+age))
bmireg_impnoage <- with(data=nhanes2_imp, lm(bmi~hyp+chl))
#type "pool.compare(bmireg_imp, bmireg_impnoage)" to see full results
pool.compare(bmireg_imp, bmireg_impnoage)[c(9:12,18)]
```

---
## Illustration: using all $m$ datasets

You also can fit logistic regressions.  For example to predict hypertension from all the other variables, do
```{r fig.height=3.3}
hyplogreg_imp <- with(data=nhanes2_imp, glm(hyp~bmi+chl+age, family = binomial))
```

This turns out to be problematic here because we have some logistic regressions with perfect predictions.  

---
## Illustration: using all $m$ datasets

```{r fig.height=3.3}
hyplogreg <- pool(hyplogreg_imp)
summary(hyplogreg)
```

We do not have enough data to do a meaningful logistic regression here, unless we drop age as a predictor, but the command structure is fine!


---
## Modifying predictors in the imputation models

Going back to the imputed datasets, which variables does `mice()` use as predictors for imputation of each incomplete variable?
```{r fig.height=3.3}
nhanes2_imp$predictorMatrix
```


---
## Modifying predictors in the imputation models

We can choose to exclude variables from any of the imputation models. For example, suppose we think that `hyp` should not predict `bmi`. Then, 
```{r fig.height=3.3}
pred <- nhanes2_imp$predictorMatrix
pred["bmi","hyp"] <- 0
pred
```
```{r eval=F}
mice(nhanes2,m=10,defaultMethod=c("norm","logreg","polyreg","polr"),predictorMatrix=pred)
```





